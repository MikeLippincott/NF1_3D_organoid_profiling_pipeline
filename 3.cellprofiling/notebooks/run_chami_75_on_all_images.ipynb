{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import tifffile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from arg_parsing_utils import check_for_missing_args, parse_args\n",
    "from file_reading import *\n",
    "from notebook_init_utils import bandicoot_check, init_notebook\n",
    "from sammed3d_featurizer import call_whole_image_sammed3d_pipeline\n",
    "from torchvision import transforms as v2\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69c0c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# get starting memory (cpu)\n",
    "start_mem = psutil.Process(os.getpid()).memory_info().rss / 1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532ad014",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir, in_notebook = init_notebook()\n",
    "\n",
    "image_base_dir = bandicoot_check(\n",
    "    pathlib.Path(os.path.expanduser(\"~/mnt/bandicoot\")).resolve(), root_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa86063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in a notebook\n"
     ]
    }
   ],
   "source": [
    "if not in_notebook:\n",
    "    args = parse_args()\n",
    "    well_fov = args[\"well_fov\"]\n",
    "    patient = args[\"patient\"]\n",
    "    input_subparent_name = args[\"input_subparent_name\"]\n",
    "    check_for_missing_args(\n",
    "        well_fov=well_fov,\n",
    "        patient=patient,\n",
    "        input_subparent_name=input_subparent_name,\n",
    "    )\n",
    "else:\n",
    "    print(\"Running in a notebook\")\n",
    "    patient = \"NF0014_T1\"\n",
    "    well_fov = \"C4-2\"\n",
    "    input_subparent_name = \"zstack_images\"\n",
    "\n",
    "\n",
    "input_dir = pathlib.Path(\n",
    "    f\"{image_base_dir}/data/{patient}/{input_subparent_name}/{well_fov}\"\n",
    ").resolve(strict=True)\n",
    "# save path\n",
    "feature_save_path = pathlib.Path(\n",
    "    f\"{image_base_dir}/data/{patient}/whole_image_features/{well_fov}_whole_image_features.parquet\"\n",
    ").resolve()\n",
    "feature_save_path.parent.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad89ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if feature_save_path.exists():\n",
    "    print(f\"Features already exist at {feature_save_path}, skipping...\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ccc7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 13:31:49.664779: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at CaicedoLab/MorphEm were not used when initializing VisionTransformer: ['head.last_layer.parametrizations.weight.original0', 'head.last_layer.parametrizations.weight.original1', 'head.mlp.0.bias', 'head.mlp.0.weight', 'head.mlp.2.bias', 'head.mlp.2.weight', 'head.mlp.4.bias', 'head.mlp.4.weight']\n",
      "- This IS expected if you are initializing VisionTransformer from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisionTransformer from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Noise Injector transformation\n",
    "\n",
    "\n",
    "class SaturationNoiseInjector(nn.Module):\n",
    "    def __init__(self, low=200, high=255):\n",
    "        super().__init__()\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        channel = x[0].clone()\n",
    "        noise = torch.empty_like(channel).uniform_(self.low, self.high)\n",
    "        mask = (channel == 255).float()\n",
    "        noise_masked = noise * mask\n",
    "        channel[channel == 255] = 0\n",
    "        channel = channel + noise_masked\n",
    "        x[0] = channel\n",
    "        return x\n",
    "\n",
    "\n",
    "# Self Normalize transformation\n",
    "class PerImageNormalize(nn.Module):\n",
    "    def __init__(self, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.instance_norm = nn.InstanceNorm2d(\n",
    "            num_features=1,\n",
    "            affine=False,\n",
    "            track_running_stats=False,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.instance_norm(x)\n",
    "        if x.shape[0] == 1:\n",
    "            x = x.squeeze(0)\n",
    "        return x\n",
    "\n",
    "\n",
    "def featurize_2D_image_w_chami75(\n",
    "    image_tensor: torch.Tensor, model: torch.nn.Module, device: torch.device\n",
    "):\n",
    "    # Bag of Channels (BoC) - process each channel independently\n",
    "    with torch.no_grad():\n",
    "        batch_feat = []\n",
    "        image_tensor = image_tensor.to(device)\n",
    "\n",
    "        for c in range(image_tensor.shape[1]):\n",
    "            # Extract single channel: (N, C, H, W) -> (N, 1, H, W)\n",
    "            single_channel = image_tensor[:, c, :, :].unsqueeze(1)\n",
    "\n",
    "            # Apply transforms\n",
    "            single_channel = transform(single_channel.squeeze(1)).unsqueeze(1)\n",
    "\n",
    "            # Extract features\n",
    "            output = model.forward_features(single_channel)\n",
    "            feat_temp = output[\"x_norm_clstoken\"].cpu().detach().numpy()\n",
    "            batch_feat.append(feat_temp)\n",
    "    return batch_feat[0]\n",
    "\n",
    "\n",
    "# load models\n",
    "sam3dmed_checkpoint_url = (\n",
    "    \"https://huggingface.co/blueyo0/SAM-Med3D/resolve/main/sam_med3d_turbo.pth\"\n",
    ")\n",
    "sam3dmed_checkpoint_path = pathlib.Path(\"../models/sam-med3d-turbo.pth\").resolve()\n",
    "if not sam3dmed_checkpoint_path.exists():\n",
    "    sam3dmed_checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    urllib.request.urlretrieve(sam3dmed_checkpoint_url, str(sam3dmed_checkpoint_path))\n",
    "\n",
    "# Load model\n",
    "device = \"cuda\"\n",
    "model = AutoModel.from_pretrained(\"CaicedoLab/MorphEm\", trust_remote_code=True)\n",
    "model.to(device).eval()\n",
    "\n",
    "# Define transforms\n",
    "transform = v2.Compose(\n",
    "    [\n",
    "        SaturationNoiseInjector(),\n",
    "        PerImageNormalize(),\n",
    "        v2.Resize(size=(224, 224), antialias=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "032ef71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all well fovs for this patient\n",
    "\n",
    "images_to_process = {\"patient\": [], \"well_fov\": [], \"image\": [], \"channel\": []}\n",
    "\n",
    "images_to_load = [x for x in input_dir.glob(\"*.tif\")]\n",
    "for image_file in images_to_load:\n",
    "    image = read_zstack_image(image_file)\n",
    "    # load the middle slice to check if there is anything there\n",
    "    mid_slice = image.shape[0] // 2\n",
    "    image_mid = image[mid_slice, :, :]\n",
    "    images_to_process[\"patient\"].append(patient)\n",
    "    images_to_process[\"well_fov\"].append(well_fov)\n",
    "    images_to_process[\"image\"].append(image_mid)\n",
    "    images_to_process[\"channel\"].append(f\"{image_file.stem.split('_')[1]}\")\n",
    "\n",
    "# Convert list of 2D images (H, W) to tensor (B, C, H, W)\n",
    "# Stack images and add channel dimension\n",
    "images = torch.stack(\n",
    "    [torch.tensor(img, dtype=torch.float32) for img in images_to_process[\"image\"]]\n",
    ")\n",
    "# images is now (B, H, W), add channel dimension -> (B, 1, H, W)\n",
    "images = images.unsqueeze(1)\n",
    "# Replicate channel 3 times to get (B, 3, H, W)\n",
    "images = images.repeat(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3597be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loading SAM-Med3D via MedIM from /home/lippincm/Documents/NF1_3D_organoid_profiling_pipeline/3.cellprofiling/models/sam-med3d-turbo.pth\n",
      "creating model SAM-Med3D\n",
      "try to load pretrained weights from /home/lippincm/Documents/NF1_3D_organoid_profiling_pipeline/3.cellprofiling/models/sam-med3d-turbo.pth\n",
      "✓ Successfully loaded pretrained SAM-Med3D-turbo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lippincm/miniforge3/envs/GFF_segmentation_nuclei/lib/python3.11/site-packages/torch/nn/modules/instancenorm.py:115: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loading SAM-Med3D via MedIM from /home/lippincm/Documents/NF1_3D_organoid_profiling_pipeline/3.cellprofiling/models/sam-med3d-turbo.pth\n",
      "creating model SAM-Med3D\n",
      "try to load pretrained weights from /home/lippincm/Documents/NF1_3D_organoid_profiling_pipeline/3.cellprofiling/models/sam-med3d-turbo.pth\n",
      "✓ Successfully loaded pretrained SAM-Med3D-turbo\n",
      "✓ Loading SAM-Med3D via MedIM from /home/lippincm/Documents/NF1_3D_organoid_profiling_pipeline/3.cellprofiling/models/sam-med3d-turbo.pth\n",
      "creating model SAM-Med3D\n",
      "try to load pretrained weights from /home/lippincm/Documents/NF1_3D_organoid_profiling_pipeline/3.cellprofiling/models/sam-med3d-turbo.pth\n",
      "✓ Successfully loaded pretrained SAM-Med3D-turbo\n",
      "✓ Loading SAM-Med3D via MedIM from /home/lippincm/Documents/NF1_3D_organoid_profiling_pipeline/3.cellprofiling/models/sam-med3d-turbo.pth\n",
      "creating model SAM-Med3D\n",
      "try to load pretrained weights from /home/lippincm/Documents/NF1_3D_organoid_profiling_pipeline/3.cellprofiling/models/sam-med3d-turbo.pth\n",
      "✓ Successfully loaded pretrained SAM-Med3D-turbo\n",
      "✓ Loading SAM-Med3D via MedIM from /home/lippincm/Documents/NF1_3D_organoid_profiling_pipeline/3.cellprofiling/models/sam-med3d-turbo.pth\n",
      "creating model SAM-Med3D\n",
      "try to load pretrained weights from /home/lippincm/Documents/NF1_3D_organoid_profiling_pipeline/3.cellprofiling/models/sam-med3d-turbo.pth\n",
      "✓ Successfully loaded pretrained SAM-Med3D-turbo\n"
     ]
    }
   ],
   "source": [
    "feature_dict = {\n",
    "    \"patient\": [],\n",
    "    \"well_fov\": [],\n",
    "    \"feature_name\": [],\n",
    "    \"feature_value\": [],\n",
    "}\n",
    "\n",
    "for image_index in range(images.shape[0]):\n",
    "    channel_id = images_to_process[\"channel\"][image_index]\n",
    "\n",
    "    image = images[image_index].cpu().numpy()\n",
    "    output_dict = call_whole_image_sammed3d_pipeline(\n",
    "        image=image,\n",
    "        SAMMed3D_model_path=str(sam3dmed_checkpoint_path),\n",
    "        feature_type=\"cls\",\n",
    "    )\n",
    "    feature_dict[\"patient\"].extend([f\"{patient}\"] * len(output_dict[\"feature_name\"]))\n",
    "    feature_dict[\"well_fov\"].extend([f\"{well_fov}\"] * len(output_dict[\"feature_name\"]))\n",
    "    feature_dict[\"feature_name\"].extend(\n",
    "        f\"{channel_id}_{feature_name}\" for feature_name in output_dict[\"feature_name\"]\n",
    "    )\n",
    "    feature_dict[\"feature_value\"].extend(output_dict[\"value\"])\n",
    "\n",
    "    batch_feat = featurize_2D_image_w_chami75(images, model, device)\n",
    "    for f_idx in range(batch_feat.shape[1]):\n",
    "        feature_name = f\"{channel_id}_CHAMI75_feature_{f_idx}\"\n",
    "        feature_value = batch_feat[image_index, f_idx]\n",
    "        feature_dict[\"patient\"].extend([f\"{patient}\"])\n",
    "        feature_dict[\"well_fov\"].extend([f\"{well_fov}\"])\n",
    "        feature_dict[\"feature_name\"].append(feature_name)\n",
    "        feature_dict[\"feature_value\"].append(feature_value)\n",
    "\n",
    "df = pd.DataFrame(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1948f017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>well_fov</th>\n",
       "      <th>405_CHAMI75_feature_0</th>\n",
       "      <th>405_CHAMI75_feature_1</th>\n",
       "      <th>405_CHAMI75_feature_10</th>\n",
       "      <th>405_CHAMI75_feature_100</th>\n",
       "      <th>405_CHAMI75_feature_101</th>\n",
       "      <th>405_CHAMI75_feature_102</th>\n",
       "      <th>405_CHAMI75_feature_103</th>\n",
       "      <th>405_CHAMI75_feature_104</th>\n",
       "      <th>...</th>\n",
       "      <th>TRANS_SAMMed3D_feature_90</th>\n",
       "      <th>TRANS_SAMMed3D_feature_91</th>\n",
       "      <th>TRANS_SAMMed3D_feature_92</th>\n",
       "      <th>TRANS_SAMMed3D_feature_93</th>\n",
       "      <th>TRANS_SAMMed3D_feature_94</th>\n",
       "      <th>TRANS_SAMMed3D_feature_95</th>\n",
       "      <th>TRANS_SAMMed3D_feature_96</th>\n",
       "      <th>TRANS_SAMMed3D_feature_97</th>\n",
       "      <th>TRANS_SAMMed3D_feature_98</th>\n",
       "      <th>TRANS_SAMMed3D_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NF0014_T1</td>\n",
       "      <td>C4-2</td>\n",
       "      <td>1.264606</td>\n",
       "      <td>-3.249125</td>\n",
       "      <td>0.482959</td>\n",
       "      <td>-0.367629</td>\n",
       "      <td>3.436962</td>\n",
       "      <td>-0.570055</td>\n",
       "      <td>-4.473512</td>\n",
       "      <td>1.091908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005945</td>\n",
       "      <td>-0.084674</td>\n",
       "      <td>-0.102848</td>\n",
       "      <td>-0.011269</td>\n",
       "      <td>0.019524</td>\n",
       "      <td>-0.131852</td>\n",
       "      <td>0.078574</td>\n",
       "      <td>0.173226</td>\n",
       "      <td>0.45716</td>\n",
       "      <td>0.144381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 3842 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     patient well_fov  405_CHAMI75_feature_0  405_CHAMI75_feature_1  \\\n",
       "0  NF0014_T1     C4-2               1.264606              -3.249125   \n",
       "\n",
       "   405_CHAMI75_feature_10  405_CHAMI75_feature_100  405_CHAMI75_feature_101  \\\n",
       "0                0.482959                -0.367629                 3.436962   \n",
       "\n",
       "   405_CHAMI75_feature_102  405_CHAMI75_feature_103  405_CHAMI75_feature_104  \\\n",
       "0                -0.570055                -4.473512                 1.091908   \n",
       "\n",
       "   ...  TRANS_SAMMed3D_feature_90  TRANS_SAMMed3D_feature_91  \\\n",
       "0  ...                  -0.005945                  -0.084674   \n",
       "\n",
       "   TRANS_SAMMed3D_feature_92  TRANS_SAMMed3D_feature_93  \\\n",
       "0                  -0.102848                  -0.011269   \n",
       "\n",
       "   TRANS_SAMMed3D_feature_94  TRANS_SAMMed3D_feature_95  \\\n",
       "0                   0.019524                  -0.131852   \n",
       "\n",
       "   TRANS_SAMMed3D_feature_96  TRANS_SAMMed3D_feature_97  \\\n",
       "0                   0.078574                   0.173226   \n",
       "\n",
       "   TRANS_SAMMed3D_feature_98  TRANS_SAMMed3D_feature_99  \n",
       "0                    0.45716                   0.144381  \n",
       "\n",
       "[1 rows x 3842 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    df.pivot_table(\n",
    "        index=[\"patient\", \"well_fov\"], columns=\"feature_name\", values=\"feature_value\"\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename_axis(None, axis=1)\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbd5136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\n",
    "    feature_save_path,\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b54ec37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 18.43561291694641 seconds\n",
      "Memory used: 2511.39453125 MB\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "# get starting memory (cpu)\n",
    "end_mem = psutil.Process(os.getpid()).memory_info().rss / 1024**2\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {end_mem - start_mem} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GFF_segmentation_nuclei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
